# ðŸŽ¯ ClickHouse å®žæˆ˜ç»ƒä¹ æ–‡æ¡£

## ðŸ“‹ å‰ç½®è¦æ±‚

ç¡®ä¿ä½ çš„ç³»ç»Ÿå·²å®‰è£…ï¼š
- Docker å’Œ Docker Compose
- Python 3.x
- curl å·¥å…·

## ðŸš€ ç¬¬ä¸€æ­¥ï¼šçŽ¯å¢ƒå‡†å¤‡å’ŒClickHouseéƒ¨ç½²

### 1.1 æ£€æŸ¥DockerçŽ¯å¢ƒ

```bash
# æ£€æŸ¥Dockeræ˜¯å¦è¿è¡Œ
docker --version
docker-compose --version

# æ£€æŸ¥DockeræœåŠ¡çŠ¶æ€
docker ps
```

### 1.2 å¯åŠ¨ClickHouseå®¹å™¨

```bash
# ç›´æŽ¥è¿è¡ŒClickHouseå®¹å™¨ï¼ˆæœ€ç®€å•çš„æ–¹å¼ï¼‰
docker run -d \
  --name clickhouse-test \
  -p 8123:8123 \
  -p 9000:9000 \
  -e CLICKHOUSE_USER=devuser \
  -e CLICKHOUSE_PASSWORD=devpass \
  -e CLICKHOUSE_DB=devdb \
  clickhouse/clickhouse-server:23
```

### 1.3 éªŒè¯ClickHouseå¯åŠ¨

```bash
# ç­‰å¾…å®¹å™¨å¯åŠ¨å®Œæˆ
sleep 10

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps | grep clickhouse

# æµ‹è¯•HTTPæŽ¥å£è¿žæŽ¥
curl -s "http://localhost:8123/ping"
# åº”è¯¥è¿”å›ž: Ok.

# æµ‹è¯•æ•°æ®åº“è¿žæŽ¥
docker exec dev-clickhouse clickhouse-client --query "SELECT version()"
# åº”è¯¥è¿”å›žç‰ˆæœ¬å·ï¼Œå¦‚: 23.8.16.16
```

### 1.4 åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æž„

```bash
# åˆ›å»ºé¡¹ç›®ä¸»ç›®å½•
mkdir -p /home/jackluo/learn/database/clickhouse
cd /home/jackluo/learn/database/clickhouse

# åˆ›å»ºç›®å½•ç»“æž„
mkdir -p {sql,data/sample,scripts/{generate,import},queries,docs}

# éªŒè¯ç›®å½•ç»“æž„
tree -L 3
```

## ðŸ—ï¸ ç¬¬äºŒæ­¥ï¼šæ•°æ®åº“å’Œè¡¨ç»“æž„åˆ›å»º

### 2.1 åˆ›å»ºæ•°æ®åº“

```bash
# åˆ›å»ºSQLæ–‡ä»¶
cat > sql/001_create_database.sql << 'EOF'
-- åˆ›å»ºåˆ†æžæ•°æ®åº“
CREATE DATABASE IF NOT EXISTS web_analytics;

-- ä½¿ç”¨æ•°æ®åº“
USE web_analytics;

-- æ˜¾ç¤ºæ‰€æœ‰æ•°æ®åº“
SHOW DATABASES;
EOF

# æ‰§è¡ŒSQLè„šæœ¬
docker exec dev-clickhouse clickhouse-client --query "CREATE DATABASE IF NOT EXISTS web_analytics"

# éªŒè¯æ•°æ®åº“åˆ›å»º
docker exec dev-clickhouse clickhouse-client --query "SHOW DATABASES"
# åº”è¯¥èƒ½çœ‹åˆ° web_analytics æ•°æ®åº“
```

### 2.2 åˆ›å»ºç½‘ç«™è®¿é—®æ—¥å¿—è¡¨

```bash
# åˆ›å»ºè¡¨ç»“æž„SQLæ–‡ä»¶
cat > sql/002_create_web_logs_table.sql << 'EOF'
-- åˆ›å»ºç½‘ç«™è®¿é—®æ—¥å¿—è¡¨
-- ä½¿ç”¨MergeTreeå¼•æ“Žï¼ŒæŒ‰æ—¥æœŸåˆ†åŒºï¼Œé€‚åˆæ—¶é—´åºåˆ—æ•°æ®åˆ†æž

CREATE TABLE web_analytics.web_logs (
    -- ä¸»é”®å’Œæ—¶é—´æˆ³
    event_time DateTime64(3) COMMENT 'äº‹ä»¶æ—¶é—´ï¼Œç²¾ç¡®åˆ°æ¯«ç§’',

    -- ç”¨æˆ·ä¿¡æ¯
    user_id String COMMENT 'ç”¨æˆ·å”¯ä¸€æ ‡è¯†',
    session_id String COMMENT 'ä¼šè¯ID',

    -- è¯·æ±‚ä¿¡æ¯
    url String COMMENT 'è®¿é—®çš„URL',
    method LowCardinality(String) COMMENT 'HTTPæ–¹æ³•',
    status_code UInt16 COMMENT 'HTTPçŠ¶æ€ç ',
    response_time UInt32 COMMENT 'å“åº”æ—¶é—´(æ¯«ç§’)',

    -- ç”¨æˆ·çŽ¯å¢ƒ
    ip_address IPv4 COMMENT 'ç”¨æˆ·IPåœ°å€',
    user_agent String COMMENT 'ç”¨æˆ·ä»£ç†',
    referer String COMMENT 'æ¥æºé¡µé¢',

    -- åœ°ç†ä½ç½®
    country FixedString(2) COMMENT 'å›½å®¶ä»£ç ',
    region FixedString(50) COMMENT 'åœ°åŒº',
    city FixedString(50) COMMENT 'åŸŽå¸‚',

    -- è®¾å¤‡ä¿¡æ¯
    device_type LowCardinality(String) COMMENT 'è®¾å¤‡ç±»åž‹',
    browser LowCardinality(String) COMMENT 'æµè§ˆå™¨',
    os LowCardinality(String) COMMENT 'æ“ä½œç³»ç»Ÿ',

    -- å†…å®¹ä¿¡æ¯
    content_type LowCardinality(String) COMMENT 'å†…å®¹ç±»åž‹',
    response_size UInt64 COMMENT 'å“åº”å¤§å°(å­—èŠ‚)',

    -- ä¸šåŠ¡å­—æ®µ
    is_new_user UInt8 DEFAULT 0 COMMENT 'æ˜¯å¦æ–°ç”¨æˆ·',
    is_bounce UInt8 DEFAULT 0 COMMENT 'æ˜¯å¦è·³å‡ºè®¿é—®',

    -- ç´¢å¼•å­—æ®µ
    date Date MATERIALIZED toDate(event_time) COMMENT 'æ—¥æœŸ(ç‰©åŒ–å­—æ®µ)',
    hour UInt8 MATERIALIZED toHour(event_time) COMMENT 'å°æ—¶(ç‰©åŒ–å­—æ®µ)'
)
ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(event_time)  -- æŒ‰æ—¥æœŸåˆ†åŒº
ORDER BY (event_time, url, user_id)  -- æŽ’åºé”®ï¼Œå½±å“æŸ¥è¯¢æ€§èƒ½
PRIMARY KEY (event_time)             -- ä¸»é”®
SETTINGS index_granularity = 8192;   -- ç´¢å¼•ç²’åº¦
EOF

# æ‰§è¡Œå»ºè¡¨è„šæœ¬
docker exec dev-clickhouse clickhouse-client --query "$(cat sql/002_create_web_logs_table.sql)"

# éªŒè¯è¡¨åˆ›å»ºæˆåŠŸ
docker exec dev-clickhouse clickhouse-client --query "SHOW TABLES FROM web_analytics"
# åº”è¯¥èƒ½çœ‹åˆ°: web_logs

# æŸ¥çœ‹è¡¨ç»“æž„è¯¦æƒ…
docker exec dev-clickhouse clickhouse-client --query "DESCRIBE web_analytics.web_logs"
```

## ðŸ“ ç¬¬ä¸‰æ­¥ï¼šæ•°æ®ç”Ÿæˆè„šæœ¬

### 3.1 åˆ›å»ºæ•°æ®ç”Ÿæˆè„šæœ¬

```bash
# åˆ›å»ºPythonæ•°æ®ç”Ÿæˆè„šæœ¬
cat > scripts/generate/generate_simple_data.py << 'EOF'
#!/usr/bin/env python3
"""
ClickHouse ç½‘ç«™è®¿é—®æ—¥å¿—æ•°æ®ç”Ÿæˆå™¨ (ç®€åŒ–ç‰ˆ)
ä¸ä¾èµ–å¤–éƒ¨åº“ï¼Œç”Ÿæˆæ¨¡æ‹Ÿçš„ç½‘ç«™è®¿é—®æ•°æ®ç”¨äºŽå­¦ä¹ 
"""

import random
import json
from datetime import datetime, timedelta

# é…ç½®å‚æ•°
RECORDS_COUNT = 5000  # ç”Ÿæˆè®°å½•æ•°é‡
START_DATE = datetime(2024, 1, 1)
END_DATE = datetime.now()

# ç½‘ç«™URLåˆ—è¡¨
URLS = [
    '/', '/home', '/about', '/products', '/contact',
    '/products/item1', '/products/item2', '/products/item3',
    '/blog', '/blog/post1', '/blog/post2', '/blog/post3',
    '/api/users', '/api/products', '/api/orders',
    '/login', '/register', '/dashboard', '/profile',
    '/search?q=laptop', '/search?q=phone', '/category/electronics',
    '/cart', '/checkout', '/help', '/faq'
]

# HTTPæ–¹æ³•åˆ†å¸ƒ
HTTP_METHODS = ['GET', 'POST', 'PUT', 'DELETE']
METHOD_WEIGHTS = [0.8, 0.15, 0.04, 0.01]

# çŠ¶æ€ç åˆ†å¸ƒ
STATUS_CODES = [200, 404, 500, 301, 302, 403]
STATUS_WEIGHTS = [0.85, 0.08, 0.02, 0.02, 0.02, 0.01]

# è®¾å¤‡ç±»åž‹
DEVICE_TYPES = ['Desktop', 'Mobile', 'Tablet']
DEVICE_WEIGHTS = [0.6, 0.35, 0.05]

# æµè§ˆå™¨
BROWSERS = ['Chrome', 'Firefox', 'Safari', 'Edge', 'Opera']
BROWSER_WEIGHTS = [0.65, 0.15, 0.12, 0.07, 0.01]

# æ“ä½œç³»ç»Ÿ
OS_LIST = ['Windows', 'macOS', 'Linux', 'Android', 'iOS']
OS_WEIGHTS = [0.4, 0.25, 0.15, 0.15, 0.05]

# å›½å®¶ä»£ç å’Œå¯¹åº”åœ°åŒº
COUNTRY_DATA = [
    ('CN', 'åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·ž', 'æ·±åœ³', 'æ­å·ž', 'æˆéƒ½', 'æ­¦æ±‰', 'è¥¿å®‰', 'å—äº¬'),
    ('US', 'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia'),
    ('JP', 'Tokyo', 'Osaka', 'Kyoto', 'Yokohama', 'Nagoya'),
    ('UK', 'London', 'Manchester', 'Birmingham', 'Liverpool', 'Edinburgh'),
    ('DE', 'Berlin', 'Munich', 'Hamburg', 'Frankfurt', 'Cologne'),
    ('FR', 'Paris', 'Lyon', 'Marseille', 'Toulouse', 'Nice'),
    ('CA', 'Toronto', 'Montreal', 'Vancouver', 'Calgary', 'Ottawa'),
    ('AU', 'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide'),
    ('KR', 'Seoul', 'Busan', 'Incheon', 'Daegu', 'Daejeon'),
    ('SG', 'Singapore')
]

def generate_random_ip():
    """ç”ŸæˆéšæœºIPåœ°å€"""
    return f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

def generate_user_agent():
    """ç”Ÿæˆç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²"""
    browsers = ['Chrome/120.0', 'Firefox/119.0', 'Safari/17.0', 'Edge/120.0']
    os_list = ['Windows NT 10.0', 'Macintosh', 'X11', 'Android 12', 'iPhone OS 17']

    browser = random.choice(browsers)
    os_name = random.choice(os_list)

    return f"Mozilla/5.0 ({os_name}) AppleWebKit/537.36 (KHTML, like Gecko) {browser} Safari/537.36"

def generate_location():
    """ç”Ÿæˆåœ°ç†ä½ç½®ä¿¡æ¯"""
    country_data = random.choice(COUNTRY_DATA)
    country = country_data[0]
    cities = country_data[1:]
    region = random.choice(cities)
    city = region  # ç®€åŒ–å¤„ç†ï¼ŒåŸŽå¸‚å’Œåœ°åŒºç›¸åŒ
    return country, region, city

def generate_single_record(record_id):
    """ç”Ÿæˆå•æ¡è®¿é—®è®°å½•"""

    # ç”Ÿæˆéšæœºæ—¶é—´
    random_seconds = random.randint(0, int((END_DATE - START_DATE).total_seconds()))
    event_time = START_DATE + timedelta(seconds=random_seconds)

    # ç”Ÿæˆç”¨æˆ·å’Œä¼šè¯ä¿¡æ¯
    user_id = f"user_{random.randint(1, 1000)}"
    session_id = f"session_{random.randint(1, 5000)}"

    # ç”Ÿæˆè¯·æ±‚ä¿¡æ¯
    url = random.choice(URLS)
    method = random.choices(HTTP_METHODS, weights=METHOD_WEIGHTS)[0]
    status_code = random.choices(STATUS_CODES, weights=STATUS_WEIGHTS)[0]
    response_time = random.randint(50, 2000)  # 50-2000ms
    response_size = random.randint(1024, 1024000)  # 1KB-1MB

    # ç”Ÿæˆç”¨æˆ·çŽ¯å¢ƒä¿¡æ¯
    ip_address = generate_random_ip()
    user_agent = generate_user_agent()
    referer = random.choice(['https://www.google.com', 'https://www.baidu.com',
                            'https://www.bing.com', '', 'https://twitter.com'])

    # ç”Ÿæˆåœ°ç†ä½ç½®
    country, region, city = generate_location()

    # ç”Ÿæˆè®¾å¤‡ä¿¡æ¯
    device_type = random.choices(DEVICE_TYPES, weights=DEVICE_WEIGHTS)[0]
    browser = random.choices(BROWSERS, weights=BROWSER_WEIGHTS)[0]
    os_name = random.choices(OS_LIST, weights=OS_WEIGHTS)[0]

    # ç”Ÿæˆå†…å®¹ç±»åž‹
    content_types = ['text/html', 'application/json', 'text/css', 'application/javascript',
                    'image/png', 'image/jpeg', 'video/mp4']
    content_type = random.choice(content_types)

    # ç”Ÿæˆä¸šåŠ¡å­—æ®µ
    is_new_user = 1 if random.random() < 0.2 else 0  # 20%æ–°ç”¨æˆ·
    is_bounce = 1 if random.random() < 0.35 else 0    # 35%è·³å‡ºçŽ‡

    return {
        'event_time': event_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3],
        'user_id': user_id,
        'session_id': session_id,
        'url': url,
        'method': method,
        'status_code': status_code,
        'response_time': response_time,
        'ip_address': ip_address,
        'user_agent': user_agent,
        'referer': referer,
        'country': country,
        'region': region,
        'city': city,
        'device_type': device_type,
        'browser': browser,
        'os': os_name,
        'content_type': content_type,
        'response_size': response_size,
        'is_new_user': is_new_user,
        'is_bounce': is_bounce
    }

def generate_csv_data():
    """ç”ŸæˆCSVæ ¼å¼çš„æ•°æ®"""

    print(f"æ­£åœ¨ç”Ÿæˆ {RECORDS_COUNT} æ¡æ¨¡æ‹Ÿæ•°æ®...")

    # CSVå¤´éƒ¨
    headers = [
        'event_time', 'user_id', 'session_id', 'url', 'method', 'status_code',
        'response_time', 'ip_address', 'user_agent', 'referer', 'country',
        'region', 'city', 'device_type', 'browser', 'os', 'content_type',
        'response_size', 'is_new_user', 'is_bounce'
    ]

    csv_lines = [','.join(headers)]  # æ·»åŠ CSVå¤´éƒ¨

    # ç”Ÿæˆæ•°æ®è¡Œ
    for i in range(RECORDS_COUNT):
        if i % 1000 == 0:
            print(f"å·²ç”Ÿæˆ {i}/{RECORDS_COUNT} æ¡è®°å½•...")

        record = generate_single_record(i)

        # è½¬ä¹‰CSVä¸­çš„ç‰¹æ®Šå­—ç¬¦
        csv_line = ','.join([
            record['event_time'],
            record['user_id'],
            record['session_id'],
            f'"{record["url"]}"',  # URLå¯èƒ½åŒ…å«é€—å·ï¼Œç”¨å¼•å·åŒ…å›´
            record['method'],
            str(record['status_code']),
            str(record['response_time']),
            record['ip_address'],
            f'"{record["user_agent"]}"',  # User AgentåŒ…å«é€—å·
            f'"{record["referer"]}"',
            record['country'],
            f'"{record["region"]}"',
            f'"{record["city"]}"',
            record['device_type'],
            record['browser'],
            record['os'],
            record['content_type'],
            str(record['response_size']),
            str(record['is_new_user']),
            str(record['is_bounce'])
        ])

        csv_lines.append(csv_line)

    return '\n'.join(csv_lines)

def main():
    """ä¸»å‡½æ•°"""
    print("=== ClickHouse ç½‘ç«™è®¿é—®æ—¥å¿—æ•°æ®ç”Ÿæˆå™¨ ===")

    # ç”ŸæˆCSVæ•°æ®
    csv_data = generate_csv_data()

    # ä¿å­˜CSVæ–‡ä»¶
    output_file = '../../data/sample/web_logs_sample.csv'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(csv_data)

    print(f"\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"ðŸ“ æ–‡ä»¶ä¿å­˜è‡³: {output_file}")
    print(f"ðŸ“Š æ€»è®°å½•æ•°: {RECORDS_COUNT}")
    print(f"ðŸ“… æ—¶é—´èŒƒå›´: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nðŸ“ˆ æ•°æ®åˆ†å¸ƒé¢„è§ˆ:")
    print(f"   - ç‹¬ç«‹ç”¨æˆ·æ•°: ~1000")
    print(f"   - ç‹¬ç«‹ä¼šè¯æ•°: ~5000")
    print(f"   - ä¸åŒé¡µé¢æ•°: {len(URLS)}")
    print(f"   - è¦†ç›–å›½å®¶æ•°: {len(COUNTRY_DATA)}")
    print(f"   - æ–°ç”¨æˆ·æ¯”ä¾‹: 20%")
    print(f"   - è·³å‡ºçŽ‡: 35%")

if __name__ == "__main__":
    main()
EOF

# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x scripts/generate/generate_simple_data.py
```

### 3.2 è¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬

```bash
# è¿›å…¥ç”Ÿæˆè„šæœ¬ç›®å½•
cd scripts/generate

# è¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬
python3 generate_simple_data.py

# é¢„æœŸè¾“å‡ºï¼š
# === ClickHouse ç½‘ç«™è®¿é—®æ—¥å¿—æ•°æ®ç”Ÿæˆå™¨ ===
# æ­£åœ¨ç”Ÿæˆ 5000 æ¡æ¨¡æ‹Ÿæ•°æ®...
# å·²ç”Ÿæˆ 0/5000 æ¡è®°å½•...
# å·²ç”Ÿæˆ 1000/5000 æ¡è®°å½•...
# ...
# âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼
# ðŸ“ æ–‡ä»¶ä¿å­˜è‡³: ../../data/sample/web_logs_sample.csv
# ðŸ“Š æ€»è®°å½•æ•°: 5000

# éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
ls -la ../../data/sample/
wc -l ../../data/sample/web_logs_sample.csv

# æŸ¥çœ‹æ–‡ä»¶å‰å‡ è¡Œ
head -5 ../../data/sample/web_logs_sample.csv
```

## ðŸ“¥ ç¬¬å››æ­¥ï¼šæ•°æ®å¯¼å…¥

### 4.1 åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

```bash
# å›žåˆ°é¡¹ç›®æ ¹ç›®å½•
cd /home/jackluo/learn/database/clickhouse

# åˆ›å»ºå°‘é‡ç¤ºä¾‹æ•°æ®ç”¨äºŽå¿«é€Ÿæµ‹è¯•
cat > sql/003_insert_sample_data.sql << 'EOF'
-- æ’å…¥ç¤ºä¾‹æ•°æ®
INSERT INTO web_analytics.web_logs VALUES
('2024-01-15 10:30:00.000', 'user_001', 'session_001', '/', 'GET', 200, 150, '192.168.1.1', 'Mozilla/5.0 (Windows NT 10.0) Chrome/120.0', 'https://www.google.com', 'CN', 'åŒ—äº¬', 'åŒ—äº¬', 'Desktop', 'Chrome', 'Windows', 'text/html', 10240, 1, 0),
('2024-01-15 10:31:00.000', 'user_001', 'session_001', '/products', 'GET', 200, 200, '192.168.1.1', 'Mozilla/5.0 (Windows NT 10.0) Chrome/120.0', 'https://example.com/', 'CN', 'åŒ—äº¬', 'åŒ—äº¬', 'Desktop', 'Chrome', 'Windows', 'text/html', 15360, 0, 0),
('2024-01-15 10:32:00.000', 'user_002', 'session_002', '/login', 'GET', 200, 100, '192.168.1.2', 'Mozilla/5.0 (iPhone OS 17) Safari/17.0', 'https://www.baidu.com', 'US', 'New York', 'New York', 'Mobile', 'Safari', 'iOS', 'text/html', 8192, 1, 0),
('2024-01-15 10:33:00.000', 'user_002', 'session_002', '/api/login', 'POST', 200, 500, '192.168.1.2', 'Mozilla/5.0 (iPhone OS 17) Safari/17.0', 'https://example.com/login', 'US', 'New York', 'New York', 'Mobile', 'Safari', 'iOS', 'application/json', 512, 0, 0),
('2024-01-15 10:34:00.000', 'user_003', 'session_003', '/products/item1', 'GET', 404, 80, '192.168.1.3', 'Mozilla/5.0 (Macintosh) Firefox/119.0', 'https://www.bing.com', 'JP', 'Tokyo', 'Tokyo', 'Desktop', 'Firefox', 'macOS', 'text/html', 2048, 1, 1);
EOF

# æ‰§è¡Œæ•°æ®æ’å…¥
docker exec dev-clickhouse clickhouse-client --query "$(cat sql/003_insert_sample_data.sql)"

# éªŒè¯æ•°æ®æ’å…¥
docker exec dev-clickhouse clickhouse-client --query "SELECT COUNT(*) FROM web_analytics.web_logs"
# åº”è¯¥è¿”å›ž: 5
```

### 4.2 å¯¼å…¥å¤§é‡CSVæ•°æ®ï¼ˆå¯é€‰ï¼‰

```bash
# åˆ›å»ºå¯¼å…¥è„šæœ¬
cat > scripts/import/import_data.sh << 'EOF'
#!/bin/bash

# ClickHouse CSVæ•°æ®å¯¼å…¥è„šæœ¬
set -e

CLICKHOUSE_CONTAINER="clickhouse-test"
DATABASE="web_analytics"
TABLE="web_logs"
CSV_FILE="../../data/sample/web_logs_sample.csv"

echo "=== ClickHouse CSVæ•°æ®å¯¼å…¥å·¥å…· ==="

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -f "$CSV_FILE" ]; then
    echo "é”™è¯¯: æ•°æ®æ–‡ä»¶ '$CSV_FILE' ä¸å­˜åœ¨ï¼"
    echo "è¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬"
    exit 1
fi

echo "æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹å¯¼å…¥..."

# è·³è¿‡å¤´éƒ¨å¯¼å…¥æ•°æ®
tail -n +2 "$CSV_FILE" | docker exec -i $CLICKHOUSE_CONTAINER clickhouse-client \
    --query "INSERT INTO $DATABASE.$TABLE FORMAT CSV"

# éªŒè¯å¯¼å…¥ç»“æžœ
total_records=$(docker exec $CLICKHOUSE_CONTAINER clickhouse-client \
    --query "SELECT COUNT(*) FROM $DATABASE.$TABLE" 2>/dev/null)

echo "âœ… æ•°æ®å¯¼å…¥å®Œæˆï¼"
echo "ðŸ“Š æ€»è®°å½•æ•°: $total_records"
EOF

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x scripts/import/import_data.sh

# è¿è¡Œå¯¼å…¥è„šæœ¬
cd scripts/import
./import_data.sh
```

## ðŸ“Š ç¬¬äº”æ­¥ï¼šæŸ¥è¯¢åˆ†æžç»ƒä¹ 

### 5.1 åŸºç¡€ç»Ÿè®¡æŸ¥è¯¢

```bash
# å›žåˆ°é¡¹ç›®æ ¹ç›®å½•
cd /home/jackluo/learn/database/clickhouse

# åˆ›å»ºåŸºç¡€ç»Ÿè®¡æŸ¥è¯¢
cat > queries/001_basic_stats.sql << 'EOF'
-- åŸºç¡€è®¿é—®ç»Ÿè®¡
SELECT COUNT(*) as total_visits FROM web_analytics.web_logs;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/001_basic_statistics.sql)"
```

### 5.2 ç”¨æˆ·ç»Ÿè®¡æŸ¥è¯¢

```bash
# åˆ›å»ºç”¨æˆ·ç»Ÿè®¡æŸ¥è¯¢
cat > queries/002_user_stats.sql << 'EOF'
-- ç”¨æˆ·ç»Ÿè®¡æŸ¥è¯¢
SELECT
    COUNT(DISTINCT user_id) as unique_users,
    COUNT(DISTINCT session_id) as unique_sessions,
    SUM(is_new_user) as new_users,
    ROUND(new_users * 100.0 / unique_users, 2) as new_user_rate
FROM web_analytics.web_logs
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/002_user_stats.sql)"
```

### 5.3 åœ°ç†åˆ†æžæŸ¥è¯¢

```bash
# åˆ›å»ºåœ°ç†åˆ†æžæŸ¥è¯¢
cat > queries/003_geo_analysis.sql << 'EOF'
-- åœ°ç†ä½ç½®åˆ†æž
SELECT
    country,
    COUNT(*) as visits,
    COUNT(DISTINCT user_id) as unique_users,
    ROUND(AVG(response_time), 2) as avg_response_time
FROM web_analytics.web_logs
GROUP BY country
ORDER BY visits DESC
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/003_geo_analysis.sql)"
```

### 5.4 è®¾å¤‡åˆ†æžæŸ¥è¯¢

```bash
# åˆ›å»ºè®¾å¤‡åˆ†æžæŸ¥è¯¢
cat > queries/004_device_analysis.sql << 'EOF'
-- è®¾å¤‡å’Œæµè§ˆå™¨åˆ†æž
SELECT
    device_type,
    browser,
    os,
    COUNT(*) as visits,
    COUNT(DISTINCT user_id) as unique_users
FROM web_analytics.web_logs
GROUP BY device_type, browser, os
ORDER BY visits DESC
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/004_device_analysis.sql)"
```

### 5.5 æŸ¥çœ‹è¡¨æ•°æ®

```bash
# æŸ¥çœ‹æ‰€æœ‰æ•°æ®
docker exec dev-clickhouse clickhouse-client --query "SELECT * FROM web_analytics.web_logs FORMAT PrettyCompact"

# æŸ¥çœ‹æœ€æ–°å‡ æ¡è®°å½•
docker exec dev-clickhouse clickhouse-client --query "SELECT * FROM web_analytics.web_logs ORDER BY event_time DESC LIMIT 3 FORMAT PrettyCompact"
```

## ðŸ› ï¸ ç¬¬å…­æ­¥ï¼šåˆ›å»ºè‡ªåŠ¨åŒ–åˆ†æžè„šæœ¬

### 6.1 åˆ›å»ºç»¼åˆåˆ†æžè„šæœ¬

```bash
# åˆ›å»ºè‡ªåŠ¨åŒ–åˆ†æžè„šæœ¬
cat > scripts/analyze_data.sh << 'EOF'
#!/bin/bash

# ClickHouse æ•°æ®åˆ†æžè„šæœ¬
set -e

CLICKHOUSE_CONTAINER="clickhouse-test"
DATABASE="web_analytics"

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}=== $1 ===${NC}"
}

log_success() {
    echo -e "${GREEN}$1${NC}"
}

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
check_container() {
    if ! docker ps | grep -q $CLICKHOUSE_CONTAINER; then
        echo "é”™è¯¯: ClickHouseå®¹å™¨æœªè¿è¡Œï¼"
        exit 1
    fi
}

# ä¸»åˆ†æžå‡½æ•°
main() {
    echo "ðŸŒ ClickHouse ç½‘ç«™è®¿é—®æ•°æ®åˆ†æžæŠ¥å‘Š"
    echo "===================================="
    echo

    check_container

    # 1. åŸºç¡€ç»Ÿè®¡
    log_info "ðŸ“Š åŸºç¡€è®¿é—®ç»Ÿè®¡"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            'æ€»è®¿é—®é‡' as metric,
            COUNT(*) as value
        FROM $DATABASE.web_logs

        UNION ALL

        SELECT
            'ç‹¬ç«‹ç”¨æˆ·æ•°',
            COUNT(DISTINCT user_id)
        FROM $DATABASE.web_logs

        UNION ALL

        SELECT
            'ç‹¬ç«‹ä¼šè¯æ•°',
            COUNT(DISTINCT session_id)
        FROM $DATABASE.web_logs

        UNION ALL

        SELECT
            'æ–°ç”¨æˆ·æ•°',
            SUM(is_new_user)
        FROM $DATABASE.web_logs

        UNION ALL

        SELECT
            'è·³å‡ºè®¿é—®æ•°',
            SUM(is_bounce)
        FROM $DATABASE.web_logs

        FORMAT PrettyCompact
    "
    echo

    # 2. åœ°ç†åˆ†å¸ƒ
    log_info "ðŸŒ åœ°ç†ä½ç½®åˆ†å¸ƒ"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            country,
            COUNT(*) as visits,
            COUNT(DISTINCT user_id) as unique_users,
            ROUND(AVG(response_time), 2) as avg_response_time_ms
        FROM $DATABASE.web_logs
        GROUP BY country
        ORDER BY visits DESC
        FORMAT PrettyCompact
    "
    echo

    # 3. è®¾å¤‡åˆ†æž
    log_info "ðŸ“± è®¾å¤‡å’Œæµè§ˆå™¨åˆ†æž"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            device_type,
            browser,
            os,
            COUNT(*) as visits,
            COUNT(DISTINCT user_id) as unique_users,
            ROUND(AVG(response_time), 2) as avg_response_time_ms
        FROM $DATABASE.web_logs
        GROUP BY device_type, browser, os
        ORDER BY visits DESC
        FORMAT PrettyCompact
    "
    echo

    # 4. çŠ¶æ€ç åˆ†æž
    log_info "ðŸ“ˆ HTTPçŠ¶æ€ç åˆ†æž"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            status_code,
            CASE
                WHEN status_code < 300 THEN 'æˆåŠŸ'
                WHEN status_code < 400 THEN 'é‡å®šå‘'
                WHEN status_code < 500 THEN 'å®¢æˆ·ç«¯é”™è¯¯'
                ELSE 'æœåŠ¡å™¨é”™è¯¯'
            END as status_type,
            COUNT(*) as count,
            ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM $DATABASE.web_logs), 2) as percentage
        FROM $DATABASE.web_logs
        GROUP BY status_code
        ORDER BY count DESC
        FORMAT PrettyCompact
    "
    echo

    # 5. å“åº”æ—¶é—´åˆ†æž
    log_info "âš¡ å“åº”æ—¶é—´åˆ†æž"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            ROUND(AVG(response_time), 2) as avg_response_time_ms,
            MIN(response_time) as min_response_time_ms,
            MAX(response_time) as max_response_time_ms,
            ROUND(quantile(0.50)(response_time), 2) as median_response_time_ms,
            ROUND(quantile(0.95)(response_time), 2) as p95_response_time_ms
        FROM $DATABASE.web_logs
        FORMAT PrettyCompact
    "
    echo

    # 6. çƒ­é—¨é¡µé¢
    log_info "ðŸ”¥ çƒ­é—¨è®¿é—®é¡µé¢"
    docker exec $CLICKHOUSE_CONTAINER clickhouse-client --query "
        SELECT
            url,
            COUNT(*) as visits,
            COUNT(DISTINCT user_id) as unique_users,
            ROUND(AVG(response_time), 2) as avg_response_time_ms
        FROM $DATABASE.web_logs
        GROUP BY url
        ORDER BY visits DESC
        LIMIT 10
        FORMAT PrettyCompact
    "
    echo

    log_success "âœ… åˆ†æžå®Œæˆï¼"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"
EOF

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x scripts/analyze_data.sh
```

### 6.2 è¿è¡Œå®Œæ•´åˆ†æž

```bash
# è¿è¡Œå®Œæ•´çš„åˆ†æžæŠ¥å‘Š
./scripts/analyze_data.sh
```

## ðŸ“š ç¬¬ä¸ƒæ­¥ï¼šæ·±å…¥ç†è§£ClickHouseç‰¹æ€§

### 7.1 æŸ¥çœ‹åˆ†åŒºä¿¡æ¯

```bash
# æŸ¥çœ‹è¡¨çš„åˆ†åŒºä¿¡æ¯
docker exec dev-clickhouse clickhouse-client --query "
SELECT
    partition,
    count() as rows,
    count() * 100.0 / (SELECT count() FROM web_analytics.web_logs) as percentage
FROM system.parts
WHERE database = 'web_analytics' AND table = 'web_logs' AND active = 1
GROUP BY partition
ORDER BY partition
FORMAT PrettyCompact"
```

### 7.2 æŸ¥çœ‹æ•°æ®åŽ‹ç¼©çŽ‡

```bash
# æŸ¥çœ‹è¡¨çš„åŽ‹ç¼©ä¿¡æ¯
docker exec dev-clickhouse clickhouse-client --query "
SELECT
    database,
    table,
    formatReadableSize(sum(bytes)) as table_size,
    formatReadableSize(sum(data_compressed_bytes)) as compressed_size,
    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed_size,
    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) as compression_ratio
FROM system.parts
WHERE database = 'web_analytics' AND table = 'web_logs' AND active = 1
GROUP BY database, table
FORMAT PrettyCompact"
```

### 7.3 æµ‹è¯•æŸ¥è¯¢æ€§èƒ½

```bash
# æµ‹è¯•ä¸åŒæŸ¥è¯¢çš„æ‰§è¡Œæ—¶é—´
echo "æµ‹è¯•æŸ¥è¯¢æ€§èƒ½..."

# ç®€å•èšåˆæŸ¥è¯¢
echo "1. ç®€å•èšåˆæŸ¥è¯¢:"
time docker exec dev-clickhouse clickhouse-client --query "
SELECT COUNT(*) FROM web_analytics.web_logs WHERE country = 'CN'" > /dev/null

# å¤æ‚åˆ†ç»„æŸ¥è¯¢
echo "2. å¤æ‚åˆ†ç»„æŸ¥è¯¢:"
time docker exec dev-clickhouse clickhouse-client --query "
SELECT device_type, browser, COUNT(*)
FROM web_analytics.web_logs
GROUP BY device_type, browser
ORDER BY COUNT(*) DESC" > /dev/null

# æ—¶é—´èŒƒå›´æŸ¥è¯¢
echo "3. æ—¶é—´èŒƒå›´æŸ¥è¯¢:"
time docker exec dev-clickhouse clickhouse-client --query "
SELECT COUNT(*)
FROM web_analytics.web_logs
WHERE event_time BETWEEN '2024-01-01' AND '2024-12-31'" > /dev/null
```

## ðŸ” ç¬¬å…«æ­¥ï¼šé«˜çº§æŸ¥è¯¢ç»ƒä¹ 

### 8.1 æ—¶é—´çª—å£åˆ†æž

```bash
# åˆ›å»ºæ—¶é—´çª—å£åˆ†æžæŸ¥è¯¢
cat > queries/005_time_window_analysis.sql << 'EOF'
-- æ—¶é—´çª—å£åˆ†æž
SELECT
    toHour(event_time) as hour,
    COUNT(*) as visits,
    COUNT(DISTINCT user_id) as unique_users,
    ROUND(AVG(response_time), 2) as avg_response_time
FROM web_analytics.web_logs
GROUP BY hour
ORDER BY hour
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/005_time_window_analysis.sql)"
```

### 8.2 ç•™å­˜çŽ‡åˆ†æž

```bash
# åˆ›å»ºç•™å­˜çŽ‡åˆ†æžæŸ¥è¯¢
cat > queries/006_retention_analysis.sql << 'EOF'
-- ç®€å•ç•™å­˜çŽ‡åˆ†æž
WITH
    first_day AS (
        SELECT
            user_id,
            toDate(min(event_time)) as first_visit_date
        FROM web_analytics.web_logs
        GROUP BY user_id
    ),
    daily_activity AS (
        SELECT
            f.user_id,
            f.first_visit_date,
            toDate(w.event_time) as activity_date,
            dateDiff('day', f.first_visit_date, toDate(w.event_time)) as day_diff
        FROM first_day f
        JOIN web_analytics.web_logs w ON f.user_id = w.user_id
        WHERE dateDiff('day', f.first_visit_date, toDate(w.event_time)) <= 7
    )
SELECT
    day_diff,
    COUNT(DISTINCT user_id) as retained_users,
    round(retained_users * 100.0 / COUNT(DISTINCT first_day.user_id), 2) as retention_rate
FROM daily_activity da
RIGHT JOIN first_day fd ON da.user_id = fd.user_id
GROUP BY day_diff
ORDER BY day_diff
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/006_retention_analysis.sql)"
```

### 8.3 æ¼æ–—åˆ†æž

```bash
# åˆ›å»ºæ¼æ–—åˆ†æžæŸ¥è¯¢
cat > queries/007_funnel_analysis.sql << 'EOF'
-- ç½‘ç«™è®¿é—®æ¼æ–—åˆ†æž
SELECT
    level,
    url_pattern,
    COUNT(DISTINCT user_id) as users,
    round(users * 100.0 / LAG(users) OVER (ORDER BY level), 2) as conversion_rate
FROM (
    SELECT
        user_id,
        CASE
            WHEN url = '/' THEN 1
            WHEN url LIKE '/products%' THEN 2
            WHEN url LIKE '/login%' THEN 3
            WHEN url LIKE '/dashboard%' THEN 4
            ELSE 5
        END as level,
        CASE
            WHEN url = '/' THEN 'é¦–é¡µ'
            WHEN url LIKE '/products%' THEN 'äº§å“é¡µ'
            WHEN url LIKE '/login%' THEN 'ç™»å½•é¡µ'
            WHEN url LIKE '/dashboard%' THEN 'ä»ªè¡¨æ¿'
            ELSE 'å…¶ä»–'
        END as url_pattern
    FROM web_analytics.web_logs
)
GROUP BY level, url_pattern
ORDER BY level
FORMAT PrettyCompact;
EOF

# æ‰§è¡ŒæŸ¥è¯¢
docker exec dev-clickhouse clickhouse-client --query "$(cat queries/007_funnel_analysis.sql)"
```

## ðŸ§¹ ç¬¬ä¹æ­¥ï¼šæ¸…ç†å’Œç»´æŠ¤

### 9.1 åˆ›å»ºé¡¹ç›®æ–‡æ¡£

```bash
# åˆ›å»ºé¡¹ç›®README
cat > README.md << 'EOF'
# ðŸŒ ClickHouse å®žæ—¶ç½‘ç«™è®¿é—®åˆ†æžç³»ç»Ÿ

## é¡¹ç›®æ¦‚è¿°
åŸºäºŽClickHouseæž„å»ºçš„å®žæ—¶ç½‘ç«™è®¿é—®æ—¥å¿—åˆ†æžç³»ç»Ÿï¼Œç”¨äºŽå­¦ä¹ ClickHouseçš„æ ¸å¿ƒç‰¹æ€§å’Œæœ€ä½³å®žè·µã€‚

## æŠ€æœ¯æ ˆ
- **æ•°æ®åº“**: ClickHouse 23.8.16.16
- **è¿žæŽ¥æ–¹å¼**: HTTPæŽ¥å£ + å®¢æˆ·ç«¯å·¥å…·
- **æ•°æ®æ ¼å¼**: CSV + JSON
- **è„šæœ¬è¯­è¨€**: Bash + Python

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ClickHouse
```bash
docker run -d --name clickhouse-test -p 8123:8123 -p 9000:9000 \
  -e CLICKHOUSE_USER=devuser -e CLICKHOUSE_PASSWORD=devpass \
  clickhouse/clickhouse-server:23
```

### 2. åˆ›å»ºæ•°æ®åº“å’Œè¡¨
```bash
docker exec dev-clickhouse clickhouse-client --query "CREATE DATABASE web_analytics"
docker exec dev-clickhouse clickhouse-client --query "$(cat sql/002_create_web_logs_table.sql)"
```

### 3. ç”Ÿæˆæµ‹è¯•æ•°æ®
```bash
cd scripts/generate && python3 generate_simple_data.py
```

### 4. æ’å…¥ç¤ºä¾‹æ•°æ®
```bash
docker exec dev-clickhouse clickhouse-client --query "$(cat sql/003_insert_sample_data.sql)"
```

### 5. è¿è¡Œåˆ†æž
```bash
./scripts/analyze_data.sh
```

## é¡¹ç›®ç»“æž„
```
clickhouse/
â”œâ”€â”€ sql/           # SQLè„šæœ¬
â”œâ”€â”€ scripts/       # å·¥å…·è„šæœ¬
â”œâ”€â”€ queries/       # æŸ¥è¯¢è„šæœ¬
â”œâ”€â”€ data/sample/   # ç¤ºä¾‹æ•°æ®
â””â”€â”€ docs/          # æ–‡æ¡£
```

## å­¦ä¹ ç›®æ ‡
- æŽŒæ¡ClickHouseæ•°æ®ç±»åž‹å’Œè¡¨å¼•æ“Ž
- ç†è§£MergeTreeç³»åˆ—å¼•æ“Žçš„ç‰¹æ€§
- å­¦ä¹ æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- å®žè·µå®žæ—¶æ•°æ®å¤„ç†æ–¹æ¡ˆ
EOF
```

### 9.2 æ¸…ç†çŽ¯å¢ƒï¼ˆå¯é€‰ï¼‰

```bash
# å¦‚æžœéœ€è¦æ¸…ç†çŽ¯å¢ƒ
echo "æ¸…ç†ClickHouseçŽ¯å¢ƒ..."

# åœæ­¢å¹¶åˆ é™¤å®¹å™¨
docker stop clickhouse-test
docker rm clickhouse-test

# åˆ é™¤é¡¹ç›®æ–‡ä»¶ï¼ˆè°¨æ…Žæ“ä½œï¼‰
# rm -rf /home/jackluo/learn/database/clickhouse

echo "çŽ¯å¢ƒæ¸…ç†å®Œæˆ"
```

## ðŸŽ¯ å­¦ä¹ æ£€æŸ¥ç‚¹

å®Œæˆä»¥ä¸Šæ‰€æœ‰æ­¥éª¤åŽï¼Œä½ åº”è¯¥æŽŒæ¡ï¼š

âœ… **ClickHouseåŸºç¡€æ“ä½œ**
- Dockerå®¹å™¨éƒ¨ç½²
- æ•°æ®åº“å’Œè¡¨åˆ›å»º
- åŸºç¡€SQLæŸ¥è¯¢

âœ… **æ•°æ®å»ºæ¨¡èƒ½åŠ›**
- MergeTreeå¼•æ“Žé…ç½®
- åˆ†åŒºç­–ç•¥è®¾è®¡
- æ•°æ®ç±»åž‹é€‰æ‹©

âœ… **æ•°æ®å¤„ç†æŠ€èƒ½**
- CSVæ•°æ®å¯¼å…¥
- æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿ
- æ•°æ®éªŒè¯æ–¹æ³•

âœ… **åˆ†æžæŸ¥è¯¢èƒ½åŠ›**
- èšåˆç»Ÿè®¡æŸ¥è¯¢
- å¤šç»´åº¦åˆ†æž
- æ€§èƒ½ä¼˜åŒ–æŸ¥è¯¢

âœ… **è‡ªåŠ¨åŒ–è„šæœ¬**
- Shellè„šæœ¬ç¼–å†™
- æ•°æ®å¤„ç†æµæ°´çº¿
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

## ðŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

1. **ç‰©åŒ–è§†å›¾**: å­¦ä¹ å®žæ—¶èšåˆé¢„è®¡ç®—
2. **é›†ç¾¤éƒ¨ç½²**: äº†è§£åˆ†å¸ƒå¼ClickHouse
3. **æµå¼å¤„ç†**: é›†æˆKafkaå®žæ—¶æ•°æ®
4. **å¯è§†åŒ–**: è¿žæŽ¥Grafanaæˆ–Tableau
5. **æ€§èƒ½è°ƒä¼˜**: æ·±å…¥å­¦ä¹ æŸ¥è¯¢ä¼˜åŒ–

æ­å–œä½ å®Œæˆäº†ClickHouseçš„å®žæˆ˜å­¦ä¹ ï¼ç»§ç»­æŽ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½å§ï¼
EOF

chmod +x /home/jackluo/learn/database/clickhouse/docs/ClickHouseå®žæˆ˜ç»ƒä¹ æ–‡æ¡£.md
```

## ðŸ“– æ–‡æ¡£è¯´æ˜Ž

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ç»ƒä¹ æ–‡æ¡£ï¼ŒåŒ…å«ï¼š

1. **è¯¦ç»†æ­¥éª¤è¯´æ˜Ž**: æ¯ä¸€æ­¥éƒ½æœ‰å…·ä½“çš„å‘½ä»¤å’Œé¢„æœŸè¾“å‡º
2. **ä»£ç ç¤ºä¾‹**: æ‰€æœ‰è„šæœ¬å’ŒSQLéƒ½å·²æä¾›å®Œæ•´ä»£ç 
3. **éªŒè¯æ–¹æ³•**: æ¯ä¸ªæ­¥éª¤éƒ½æœ‰éªŒè¯å‘½ä»¤ç¡®ä¿æ“ä½œæˆåŠŸ
4. **å­¦ä¹ è¦ç‚¹**: æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å­¦ä¹ ç›®æ ‡å’ŒçŸ¥è¯†ç‚¹è§£é‡Š

### ðŸ“‹ æ–‡æ¡£ä½ç½®
```
/home/jackluo/learn/database/clickhouse/docs/ClickHouseå®žæˆ˜ç»ƒä¹ æ–‡æ¡£.md
```

### ðŸŽ¯ ä½¿ç”¨æ–¹æ³•
1. æŒ‰ç…§æ–‡æ¡£é¡ºåºé€æ­¥æ‰§è¡Œ
2. æ¯ä¸ªæ­¥éª¤éƒ½æä¾›äº†å®Œæ•´çš„å‘½ä»¤
3. å¯ä»¥å¤åˆ¶ç²˜è´´å‘½ä»¤ç›´æŽ¥æ‰§è¡Œ
4. åŒ…å«äº†éªŒè¯å‘½ä»¤ç¡®ä¿æ“ä½œæ­£ç¡®

### âš ï¸ æ³¨æ„äº‹é¡¹
- ç¡®ä¿DockerçŽ¯å¢ƒæ­£å¸¸è¿è¡Œ
- æŒ‰é¡ºåºæ‰§è¡Œï¼Œä¸è¦è·³è¿‡æ­¥éª¤
- é‡åˆ°é”™è¯¯æ—¶æ£€æŸ¥å‰é¢çš„æ­¥éª¤æ˜¯å¦å®Œæˆ
- æ–‡æ¡£ä¸­æä¾›äº†æ¸…ç†å‘½ä»¤å¯ä»¥é‡æ–°å¼€å§‹

çŽ°åœ¨ä½ å¯ä»¥æŒ‰ç…§è¿™ä¸ªè¯¦ç»†çš„æ–‡æ¡£ä¸€æ­¥ä¸€æ­¥ç»ƒä¹ ClickHouseçš„å®žæˆ˜æ“ä½œäº†ï¼