{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 与ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM的调用 ，这里以openai为例\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 从环境变量或配置文件中读取 API 密钥和 URL\n",
    "import os\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    model = \"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "pm = \"你好\"\n",
    "result = llm.predict(pm)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 chatmodels, 以 openai 为例\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import ChatMessage,HumanMessage,AIMessage\n",
    "\n",
    "# 初始化 ChatOpenAI 模型\n",
    "chat = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    # stream=True,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# 构建消息列表\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"你是一个友好的助手\"),\n",
    "    ChatMessage(role=\"user\", content=\"你好\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"\")\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"你是谁\"),\n",
    "    AIMessage(content=\"\")\n",
    "]\n",
    "messages = [\n",
    "    AIMessage(content=\"你好,我是jack的人工智能助手\"),\n",
    "    HumanMessage(content=\"你好,我是jack\"),\n",
    "    HumanMessage(content=\"我是谁\"),\n",
    "]\n",
    "\n",
    "# 使用 generate 方法生成对话\n",
    "response = chat.generate([messages])\n",
    "for chunk in response.generations:\n",
    "    print(chunk)\n",
    "# print(response)\n",
    "# 打印生成的对话内容\n",
    "# 处理流式输出\n",
    "# for chunk in response:\n",
    "#     for message in chunk.messages:\n",
    "#         print(message.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  流式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 类大模型的输出方法\n",
    "from langchain.llms import OpenAI\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    model = \"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "pm = \"你好\"\n",
    "\n",
    "for chunk in llm.stream(pm):    \n",
    "    print(chunk,end=\"\",flush=False)\n",
    "    # print(chunk.content, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatmodels 的流式调用方法\n",
    "# 使用clade模型\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")   # 引入消息类型\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    model = \"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"写一首春天的诗\"):    \n",
    "    print(chunk,end=\"\",flush=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追踪Token的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    # model=\"THUDM/glm-4-9b-chat\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.stream(\"写一道春天的诗\")\n",
    "    for chunk in response:\n",
    "        print(chunk, end='')\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    # model=\"THUDM/glm-4-9b-chat\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.stream(\"写一道春天的诗\")\n",
    "    for chunk in response:\n",
    "        print(chunk, end='')\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    # model=\"THUDM/glm-4-9b-chat\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.stream(\"写一道春天的诗\")\n",
    "    for chunk in response:\n",
    "        print(chunk, end='')\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义输出 \n",
    "- 输出函数参数\n",
    "- 输出json\n",
    "- 输出List\n",
    "- 输出日期\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讲笑话机器人：希望每次根据指令，可以输出一个这样的笑话（小明是怎么死的?笨死的）\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import Dict, Any  # 导入所需的类型提示\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    # model=\"THUDM/glm-4-9b-chat\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# 定义数据模型，用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"设置笑话的问题\")\n",
    "    punchline: str = Field(description=\"回答笑话的答案\")\n",
    "\n",
    "    # 验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls, v):\n",
    "        if \"?\" not in v:\n",
    "            raise ValueError(\"不符合预期的问题格式！\")\n",
    "        return v\n",
    "    \n",
    "\n",
    "# 将Joke 数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"回答用户的输入。\\n{format_instrc}\\n{query}\\n\",\n",
    "    input_variables=[\"format_instrc\", \"query\"],\n",
    "    partial_variables={\"format_instrc\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 进行管道操作\n",
    "prompt_and_model = prompt | llm\n",
    "\n",
    "output = prompt_and_model.invoke(format_instrc=\"写一个笑话\", query=\"小明是怎么死的?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U pydantic\n",
    "! pip install --upgrade langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 定义 Pydantic 数据模型（必须继承 BaseModel）\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punchline: str\n",
    "\n",
    "# 初始化解析器\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 创建提示模板（注意模板中的变量名）\n",
    "prompt = PromptTemplate(\n",
    "    template=\"回答用户问题，按格式返回一个笑话。\\n{format_instructions}\\n用户问题：{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 调用模型\n",
    "# model = ChatOpenAI()\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "llm = OpenAI(\n",
    "    # model=\"THUDM/glm-4-9b-chat\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    temperature=0,    \n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    stream=True,\n",
    "    max_tokens=1000\n",
    ")\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 测试调用\n",
    "result = chain.invoke({\"query\": \"讲一个关于程序员的笑话\"})\n",
    "print(result.dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
