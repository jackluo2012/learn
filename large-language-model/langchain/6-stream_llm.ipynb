{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同步流式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你好👋！我是人工智能助手，很高兴见到你，有什么可以帮助你的吗？"
     ]
    }
   ],
   "source": [
    "from langchain_openai import  ChatOpenAI\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "chunks = []\n",
    "for chunk in model.stream(\"你好\"):    \n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步流式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "\n",
      "\n",
      "在一个宁静的小镇上，住着一对青梅竹马的恋人，小明和小红。他们的故事从童年开始，就像两颗种子，在同一个土壤中生根发芽。\n",
      "\n",
      "小明是个聪明好学的孩子，他总是带着一颗好奇的心去探索世界。小红则是个温柔善良的女孩，她的笑容总能照亮周围的人。他们一起在小镇的街头巷尾玩耍，一起在学校的操场上奔跑，一起在图书馆里阅读。\n",
      "\n",
      "随着时间的流逝，小明和小红的感情越发深厚。他们一起度过了许多难忘的时光，比如在夏夜的星空下许下愿望，在冬日的雪地里留下足迹。他们的爱情就像小镇的风景一样，简单而美好。\n",
      "\n",
      "然而，生活总是充满了变数。小明因为家庭原因，需要去外地求学。他们不得不面临分别，小明心中充满了不舍，但他知道，这是成长的必经之路。小红则坚定地告诉小明：“无论你走到哪里，我都会在这里等你。”\n",
      "\n",
      "小明在外地努力学习，而小红则在家乡默默支持着他。他们通过书信保持着联系，每一封信都承载着他们对彼此的思念和鼓励。小明在信中写道：“我想象着有一天，我们会在同一个地方，一起看着夕阳西下。”\n",
      "\n",
      "终于，小明学成归来。他带着满心的喜悦和期待回到了小镇。那天，小红站在车站的月台上，远远地就看到了那个熟悉的身影。她的心跳加速，眼中闪烁着泪光。\n",
      "\n",
      "小明走过来，紧紧地抱住了小红。他们相视而笑，仿佛时间在这一刻静止。小明说：“我终于回来了，我们可以"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jackluo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:300: RuntimeWarning: coroutine 'async_stream' was never awaited\n",
      "  key = super().register(fileobj, events, data)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一起实现我们的梦想了。”\n",
      "\n",
      "他们决定一起开一家书店，将小镇的角落变成一个充满知识和温暖的地方。小红负责整理书籍，小明则负责接待顾客。他们的书店很快成为了小镇的标志，吸引了许多爱书之人。\n",
      "\n",
      "然而，命运似乎总爱开玩笑。一天，小红在整理书籍时不小心摔倒了，腿部受了重伤。小明焦急万分，他放下书店的一切，全心全意地照顾小红。在漫长的康复过程中，小明和小红的感情更加深厚。\n",
      "\n",
      "经过一段时间的治疗，小红终于可以下床走路了。她看着小明，眼中充满了感激：“谢谢你，小明。没有你，我可能无法走出这段阴影。”\n",
      "\n",
      "小明微笑着说：“傻瓜，我爱你，这是我应该做的。”\n",
      "\n",
      "从那以后，小明和小红过上了幸福的生活。他们的书店成为了小镇的文化中心，他们的爱情故事也成为了小镇的传说。每当夜幕降临，小镇的居民都会在窗前看着星空，轻轻地说：“愿每一对恋人都能像小明和小红一样，拥有永恒的爱情。”"
     ]
    }
   ],
   "source": [
    "# 安装 nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的故事\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "async def async_stream():\n",
    "    chunks = []\n",
    "    # 直接从 model 调用 async_stream 方法\n",
    "    async for chunk in chain.astream({\"topic\": \"爱情\"}):\n",
    "        parsed_output = output_parser.parse(chunk)\n",
    "        chunks.append(parsed_output)\n",
    "        print(parsed_output, end=\"\", flush=True)\n",
    "\n",
    "# 运行异步流处理\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json输出流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 6}]}\n",
      "{'countries': [{'name': 'France', 'population': 670}]}\n",
      "{'countries': [{'name': 'France', 'population': 670000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 4}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 460}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 460000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126000000}]}\n"
     ]
    }
   ],
   "source": [
    "# 安装 nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的故事\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# chain = prompt | model | output_parser\n",
    "chain = (model | JsonOutputParser())\n",
    "\n",
    "async def async_stream():\n",
    "    # 直接从 model 调用 async_stream 方法\n",
    "    async for chunk in chain.astream(\"以JSON 格式输出法国、西班牙和日本的国家及其人口列表。\"\n",
    "                                     '使用一个带有”countries”外部键的字典，其中包含国家列表。'\n",
    "                                     \"每个国家都应该有键`name`和`population`\"):\n",
    "        print(chunk, flush=True)\n",
    "\n",
    "# 运行异步流处理\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聊天模型产生的事件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessageChunk(content=\"\\nHello 👋! I'm ChatGLM, the artificial intelligence assistant, nice to meet you. Feel free to ask me any questions.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'THUDM/glm-4-9b-chat'}, id='run-aaa54fba-5ae5-41a1-b3f0-f939e7b1b6dc', usage_metadata={'input_tokens': 180, 'output_tokens': 461, 'total_tokens': 641, 'input_token_details': {}, 'output_token_details': {}})}, 'run_id': 'aaa54fba-5ae5-41a1-b3f0-f939e7b1b6dc', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'THUDM/glm-4-9b-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 100}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "# 安装 nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的故事\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "async def async_stream():\n",
    "    events = []\n",
    "    async for event in model.astream_events(\"hello\",version=\"v2\"):\n",
    "        events.append(event)\n",
    "    print(event)\n",
    "\n",
    "# 运行异步流处理\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "content='\\n' additional_kwargs={} response_metadata={} id='run-a250a4dc-e243-43ad-918c-45ba7b2e0e20' usage_metadata={'input_tokens': 10, 'output_tokens': 1, 'total_tokens': 11, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "好的，来content='\\n' additional_kwargs={} response_metadata={} id='run-367b94e8-e0a6-4582-8e05-b075a865db37' usage_metadata={'input_tokens': 9, 'output_tokens': 1, 'total_tokens': 10, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "天空一个简单的的颜色笑话通常是蓝色：\n",
      "\n",
      "有一天，，这是因为一只大气中的蚂蚁在气体和森林里悬浮散步颗粒，散射突然太阳它看到光中的一只蓝色大象光在河边。喝水然而。，天空蚂蚁的颜色心想：“也会哇因，时间大象、那么大天气，、一定地理位置和很大气聪明条件。”于是等因素，而它有所不同决定。去问例如大象，在一个问题日出。\n",
      "\n",
      "或蚂蚁走到日落大象时面前，，天空小心翼翼可能会地问：“呈现出大象橙色先生、红色，请问或您紫色知道。世界上在什么东西阴最天硬或吗雾？天”\n",
      "\n",
      "，大象天空想了可能想显得，灰回答暗道：“当然。此外知道，世界上最硬，的东西是钻石在高。”\n",
      "\n",
      "蚂蚁又问：“海拔那地区请问，天空，可能因为稀薄的大气而世界上显得更加什么东西湛蓝。最软呢？”\n",
      "\n",
      "大象回答：“这个嘛，我也知道，世界上最软的东西是棉花。”\n",
      "\n",
      "蚂蚁高兴地说：“谢谢大象先生，那请问，世界上什么东西最聪明呢？”\n",
      "\n",
      "大象想了想，说：“这个嘛，我得想想……”\n",
      "\n",
      "蚂蚁等啊等啊，终于等得不耐烦了，它说：“哎呀，大象先生，您怎么还不回答呢？”\n",
      "\n",
      "大象说：“哎呀，不好意思，我忘了，世界上最聪明的东西是——你这只问问题的蚂蚁！”\n",
      "\n",
      "哈哈，这个笑话有点自嘲的意味，希望您喜欢！"
     ]
    }
   ],
   "source": [
    "# 安装 nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "async def task1():\n",
    "    model = ChatOpenAI(\n",
    "        model=\"THUDM/glm-4-9b-chat\",\n",
    "        temperature=0,\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    chunks = []\n",
    "    \n",
    "    async for chunk in model.astream(\"天空是什么颜色？\"):\n",
    "        chunks.append(chunk)\n",
    "        # 判断chunks 长度为1的时候，打印chunks[0]\n",
    "        if len(chunks) == 2:\n",
    "            print(chunks[1])\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "async def task2():\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model=\"THUDM/glm-4-9b-chat\",\n",
    "        temperature=0,\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    chunks = []\n",
    "    async for chunk in model.astream(\"给我讲个笑话？\"):\n",
    "        chunks.append(chunk)\n",
    "        # 判断chunks 长度为1的时候，打印chunks[0]\n",
    "        if len(chunks) == 2:\n",
    "            print(chunks[1])\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "async def main():\n",
    "    # 同步调用\n",
    "    # await task1()\n",
    "    # await task2()\n",
    "    # 异步调用，并发执行\n",
    "    await asyncio.gather(task1(), task2())\n",
    "    \n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
