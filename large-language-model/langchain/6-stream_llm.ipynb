{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŒæ­¥æµå¼è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
     ]
    }
   ],
   "source": [
    "from langchain_openai import  ChatOpenAI\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "chunks = []\n",
    "for chunk in model.stream(\"ä½ å¥½\"):    \n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼‚æ­¥æµå¼è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "\n",
      "\n",
      "åœ¨ä¸€ä¸ªå®é™çš„å°é•‡ä¸Šï¼Œä½ç€ä¸€å¯¹é’æ¢…ç«¹é©¬çš„æ‹äººï¼Œå°æ˜å’Œå°çº¢ã€‚ä»–ä»¬çš„æ•…äº‹ä»ç«¥å¹´å¼€å§‹ï¼Œå°±åƒä¸¤é¢—ç§å­ï¼Œåœ¨åŒä¸€ä¸ªåœŸå£¤ä¸­ç”Ÿæ ¹å‘èŠ½ã€‚\n",
      "\n",
      "å°æ˜æ˜¯ä¸ªèªæ˜å¥½å­¦çš„å­©å­ï¼Œä»–æ€»æ˜¯å¸¦ç€ä¸€é¢—å¥½å¥‡çš„å¿ƒå»æ¢ç´¢ä¸–ç•Œã€‚å°çº¢åˆ™æ˜¯ä¸ªæ¸©æŸ”å–„è‰¯çš„å¥³å­©ï¼Œå¥¹çš„ç¬‘å®¹æ€»èƒ½ç…§äº®å‘¨å›´çš„äººã€‚ä»–ä»¬ä¸€èµ·åœ¨å°é•‡çš„è¡—å¤´å··å°¾ç©è€ï¼Œä¸€èµ·åœ¨å­¦æ ¡çš„æ“åœºä¸Šå¥”è·‘ï¼Œä¸€èµ·åœ¨å›¾ä¹¦é¦†é‡Œé˜…è¯»ã€‚\n",
      "\n",
      "éšç€æ—¶é—´çš„æµé€ï¼Œå°æ˜å’Œå°çº¢çš„æ„Ÿæƒ…è¶Šå‘æ·±åšã€‚ä»–ä»¬ä¸€èµ·åº¦è¿‡äº†è®¸å¤šéš¾å¿˜çš„æ—¶å…‰ï¼Œæ¯”å¦‚åœ¨å¤å¤œçš„æ˜Ÿç©ºä¸‹è®¸ä¸‹æ„¿æœ›ï¼Œåœ¨å†¬æ—¥çš„é›ªåœ°é‡Œç•™ä¸‹è¶³è¿¹ã€‚ä»–ä»¬çš„çˆ±æƒ…å°±åƒå°é•‡çš„é£æ™¯ä¸€æ ·ï¼Œç®€å•è€Œç¾å¥½ã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œç”Ÿæ´»æ€»æ˜¯å……æ»¡äº†å˜æ•°ã€‚å°æ˜å› ä¸ºå®¶åº­åŸå› ï¼Œéœ€è¦å»å¤–åœ°æ±‚å­¦ã€‚ä»–ä»¬ä¸å¾—ä¸é¢ä¸´åˆ†åˆ«ï¼Œå°æ˜å¿ƒä¸­å……æ»¡äº†ä¸èˆï¼Œä½†ä»–çŸ¥é“ï¼Œè¿™æ˜¯æˆé•¿çš„å¿…ç»ä¹‹è·¯ã€‚å°çº¢åˆ™åšå®šåœ°å‘Šè¯‰å°æ˜ï¼šâ€œæ— è®ºä½ èµ°åˆ°å“ªé‡Œï¼Œæˆ‘éƒ½ä¼šåœ¨è¿™é‡Œç­‰ä½ ã€‚â€\n",
      "\n",
      "å°æ˜åœ¨å¤–åœ°åŠªåŠ›å­¦ä¹ ï¼Œè€Œå°çº¢åˆ™åœ¨å®¶ä¹¡é»˜é»˜æ”¯æŒç€ä»–ã€‚ä»–ä»¬é€šè¿‡ä¹¦ä¿¡ä¿æŒç€è”ç³»ï¼Œæ¯ä¸€å°ä¿¡éƒ½æ‰¿è½½ç€ä»–ä»¬å¯¹å½¼æ­¤çš„æ€å¿µå’Œé¼“åŠ±ã€‚å°æ˜åœ¨ä¿¡ä¸­å†™é“ï¼šâ€œæˆ‘æƒ³è±¡ç€æœ‰ä¸€å¤©ï¼Œæˆ‘ä»¬ä¼šåœ¨åŒä¸€ä¸ªåœ°æ–¹ï¼Œä¸€èµ·çœ‹ç€å¤•é˜³è¥¿ä¸‹ã€‚â€\n",
      "\n",
      "ç»ˆäºï¼Œå°æ˜å­¦æˆå½’æ¥ã€‚ä»–å¸¦ç€æ»¡å¿ƒçš„å–œæ‚¦å’ŒæœŸå¾…å›åˆ°äº†å°é•‡ã€‚é‚£å¤©ï¼Œå°çº¢ç«™åœ¨è½¦ç«™çš„æœˆå°ä¸Šï¼Œè¿œè¿œåœ°å°±çœ‹åˆ°äº†é‚£ä¸ªç†Ÿæ‚‰çš„èº«å½±ã€‚å¥¹çš„å¿ƒè·³åŠ é€Ÿï¼Œçœ¼ä¸­é—ªçƒç€æ³ªå…‰ã€‚\n",
      "\n",
      "å°æ˜èµ°è¿‡æ¥ï¼Œç´§ç´§åœ°æŠ±ä½äº†å°çº¢ã€‚ä»–ä»¬ç›¸è§†è€Œç¬‘ï¼Œä»¿ä½›æ—¶é—´åœ¨è¿™ä¸€åˆ»é™æ­¢ã€‚å°æ˜è¯´ï¼šâ€œæˆ‘ç»ˆäºå›æ¥äº†ï¼Œæˆ‘ä»¬å¯ä»¥"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jackluo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:300: RuntimeWarning: coroutine 'async_stream' was never awaited\n",
      "  key = super().register(fileobj, events, data)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸€èµ·å®ç°æˆ‘ä»¬çš„æ¢¦æƒ³äº†ã€‚â€\n",
      "\n",
      "ä»–ä»¬å†³å®šä¸€èµ·å¼€ä¸€å®¶ä¹¦åº—ï¼Œå°†å°é•‡çš„è§’è½å˜æˆä¸€ä¸ªå……æ»¡çŸ¥è¯†å’Œæ¸©æš–çš„åœ°æ–¹ã€‚å°çº¢è´Ÿè´£æ•´ç†ä¹¦ç±ï¼Œå°æ˜åˆ™è´Ÿè´£æ¥å¾…é¡¾å®¢ã€‚ä»–ä»¬çš„ä¹¦åº—å¾ˆå¿«æˆä¸ºäº†å°é•‡çš„æ ‡å¿—ï¼Œå¸å¼•äº†è®¸å¤šçˆ±ä¹¦ä¹‹äººã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œå‘½è¿ä¼¼ä¹æ€»çˆ±å¼€ç©ç¬‘ã€‚ä¸€å¤©ï¼Œå°çº¢åœ¨æ•´ç†ä¹¦ç±æ—¶ä¸å°å¿ƒæ‘”å€’äº†ï¼Œè…¿éƒ¨å—äº†é‡ä¼¤ã€‚å°æ˜ç„¦æ€¥ä¸‡åˆ†ï¼Œä»–æ”¾ä¸‹ä¹¦åº—çš„ä¸€åˆ‡ï¼Œå…¨å¿ƒå…¨æ„åœ°ç…§é¡¾å°çº¢ã€‚åœ¨æ¼«é•¿çš„åº·å¤è¿‡ç¨‹ä¸­ï¼Œå°æ˜å’Œå°çº¢çš„æ„Ÿæƒ…æ›´åŠ æ·±åšã€‚\n",
      "\n",
      "ç»è¿‡ä¸€æ®µæ—¶é—´çš„æ²»ç–—ï¼Œå°çº¢ç»ˆäºå¯ä»¥ä¸‹åºŠèµ°è·¯äº†ã€‚å¥¹çœ‹ç€å°æ˜ï¼Œçœ¼ä¸­å……æ»¡äº†æ„Ÿæ¿€ï¼šâ€œè°¢è°¢ä½ ï¼Œå°æ˜ã€‚æ²¡æœ‰ä½ ï¼Œæˆ‘å¯èƒ½æ— æ³•èµ°å‡ºè¿™æ®µé˜´å½±ã€‚â€\n",
      "\n",
      "å°æ˜å¾®ç¬‘ç€è¯´ï¼šâ€œå‚»ç“œï¼Œæˆ‘çˆ±ä½ ï¼Œè¿™æ˜¯æˆ‘åº”è¯¥åšçš„ã€‚â€\n",
      "\n",
      "ä»é‚£ä»¥åï¼Œå°æ˜å’Œå°çº¢è¿‡ä¸Šäº†å¹¸ç¦çš„ç”Ÿæ´»ã€‚ä»–ä»¬çš„ä¹¦åº—æˆä¸ºäº†å°é•‡çš„æ–‡åŒ–ä¸­å¿ƒï¼Œä»–ä»¬çš„çˆ±æƒ…æ•…äº‹ä¹Ÿæˆä¸ºäº†å°é•‡çš„ä¼ è¯´ã€‚æ¯å½“å¤œå¹•é™ä¸´ï¼Œå°é•‡çš„å±…æ°‘éƒ½ä¼šåœ¨çª—å‰çœ‹ç€æ˜Ÿç©ºï¼Œè½»è½»åœ°è¯´ï¼šâ€œæ„¿æ¯ä¸€å¯¹æ‹äººéƒ½èƒ½åƒå°æ˜å’Œå°çº¢ä¸€æ ·ï¼Œæ‹¥æœ‰æ°¸æ’çš„çˆ±æƒ…ã€‚â€"
     ]
    }
   ],
   "source": [
    "# å®‰è£… nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "async def async_stream():\n",
    "    chunks = []\n",
    "    # ç›´æ¥ä» model è°ƒç”¨ async_stream æ–¹æ³•\n",
    "    async for chunk in chain.astream({\"topic\": \"çˆ±æƒ…\"}):\n",
    "        parsed_output = output_parser.parse(chunk)\n",
    "        chunks.append(parsed_output)\n",
    "        print(parsed_output, end=\"\", flush=True)\n",
    "\n",
    "# è¿è¡Œå¼‚æ­¥æµå¤„ç†\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jsonè¾“å‡ºæµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 6}]}\n",
      "{'countries': [{'name': 'France', 'population': 670}]}\n",
      "{'countries': [{'name': 'France', 'population': 670000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 4}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 460}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 460000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67000000}, {'name': 'Spain', 'population': 46000000}, {'name': 'Japan', 'population': 126000000}]}\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# chain = prompt | model | output_parser\n",
    "chain = (model | JsonOutputParser())\n",
    "\n",
    "async def async_stream():\n",
    "    # ç›´æ¥ä» model è°ƒç”¨ async_stream æ–¹æ³•\n",
    "    async for chunk in chain.astream(\"ä»¥JSON æ ¼å¼è¾“å‡ºæ³•å›½ã€è¥¿ç­ç‰™å’Œæ—¥æœ¬çš„å›½å®¶åŠå…¶äººå£åˆ—è¡¨ã€‚\"\n",
    "                                     'ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰â€countriesâ€å¤–éƒ¨é”®çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«å›½å®¶åˆ—è¡¨ã€‚'\n",
    "                                     \"æ¯ä¸ªå›½å®¶éƒ½åº”è¯¥æœ‰é”®`name`å’Œ`population`\"):\n",
    "        print(chunk, flush=True)\n",
    "\n",
    "# è¿è¡Œå¼‚æ­¥æµå¤„ç†\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### èŠå¤©æ¨¡å‹äº§ç”Ÿçš„äº‹ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessageChunk(content=\"\\nHello ğŸ‘‹! I'm ChatGLM, the artificial intelligence assistant, nice to meet you. Feel free to ask me any questions.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'THUDM/glm-4-9b-chat'}, id='run-aaa54fba-5ae5-41a1-b3f0-f939e7b1b6dc', usage_metadata={'input_tokens': 180, 'output_tokens': 461, 'total_tokens': 641, 'input_token_details': {}, 'output_token_details': {}})}, 'run_id': 'aaa54fba-5ae5-41a1-b3f0-f939e7b1b6dc', 'name': 'ChatOpenAI', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'THUDM/glm-4-9b-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 100}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    model=\"THUDM/glm-4-9b-chat\",\n",
    "    temperature=0,\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=api_key,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "async def async_stream():\n",
    "    events = []\n",
    "    async for event in model.astream_events(\"hello\",version=\"v2\"):\n",
    "        events.append(event)\n",
    "    print(event)\n",
    "\n",
    "# è¿è¡Œå¼‚æ­¥æµå¤„ç†\n",
    "asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤šçº¿ç¨‹è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nest_asyncio in d:\\works\\learn\\.venv\\lib\\site-packages (1.6.0)\n",
      "content='\\n' additional_kwargs={} response_metadata={} id='run-a250a4dc-e243-43ad-918c-45ba7b2e0e20' usage_metadata={'input_tokens': 10, 'output_tokens': 1, 'total_tokens': 11, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "å¥½çš„ï¼Œæ¥content='\\n' additional_kwargs={} response_metadata={} id='run-367b94e8-e0a6-4582-8e05-b075a865db37' usage_metadata={'input_tokens': 9, 'output_tokens': 1, 'total_tokens': 10, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "å¤©ç©ºä¸€ä¸ªç®€å•çš„çš„é¢œè‰²ç¬‘è¯é€šå¸¸æ˜¯è“è‰²ï¼š\n",
      "\n",
      "æœ‰ä¸€å¤©ï¼Œï¼Œè¿™æ˜¯å› ä¸ºä¸€åªå¤§æ°”ä¸­çš„èš‚èšåœ¨æ°”ä½“å’Œæ£®æ—é‡Œæ‚¬æµ®æ•£æ­¥é¢—ç²’ï¼Œæ•£å°„çªç„¶å¤ªé˜³å®ƒçœ‹åˆ°å…‰ä¸­çš„ä¸€åªè“è‰²å¤§è±¡å…‰åœ¨æ²³è¾¹ã€‚å–æ°´ç„¶è€Œã€‚ï¼Œå¤©ç©ºèš‚èšçš„é¢œè‰²å¿ƒæƒ³ï¼šâ€œä¹Ÿä¼šå“‡å› ï¼Œæ—¶é—´å¤§è±¡ã€é‚£ä¹ˆå¤§å¤©æ°”ï¼Œã€ä¸€å®šåœ°ç†ä½ç½®å’Œå¾ˆå¤§æ°”èªæ˜æ¡ä»¶ã€‚â€äºæ˜¯ç­‰å› ç´ ï¼Œè€Œå®ƒæœ‰æ‰€ä¸åŒå†³å®šã€‚å»é—®ä¾‹å¦‚å¤§è±¡ï¼Œåœ¨ä¸€ä¸ªé—®é¢˜æ—¥å‡ºã€‚\n",
      "\n",
      "æˆ–èš‚èšèµ°åˆ°æ—¥è½å¤§è±¡æ—¶é¢å‰ï¼Œï¼Œå¤©ç©ºå°å¿ƒç¿¼ç¿¼å¯èƒ½ä¼šåœ°é—®ï¼šâ€œå‘ˆç°å‡ºå¤§è±¡æ©™è‰²å…ˆç”Ÿã€çº¢è‰²ï¼Œè¯·é—®æˆ–æ‚¨ç´«è‰²çŸ¥é“ã€‚ä¸–ç•Œä¸Šåœ¨ä»€ä¹ˆä¸œè¥¿é˜´æœ€å¤©ç¡¬æˆ–å—é›¾ï¼Ÿå¤©â€\n",
      "\n",
      "ï¼Œå¤§è±¡å¤©ç©ºæƒ³äº†å¯èƒ½æƒ³æ˜¾å¾—ï¼Œç°å›ç­”æš—é“ï¼šâ€œå½“ç„¶ã€‚æ­¤å¤–çŸ¥é“ï¼Œä¸–ç•Œä¸Šæœ€ç¡¬ï¼Œçš„ä¸œè¥¿æ˜¯é’»çŸ³åœ¨é«˜ã€‚â€\n",
      "\n",
      "èš‚èšåˆé—®ï¼šâ€œæµ·æ‹”é‚£åœ°åŒºè¯·é—®ï¼Œå¤©ç©ºï¼Œå¯èƒ½å› ä¸ºç¨€è–„çš„å¤§æ°”è€Œä¸–ç•Œä¸Šæ˜¾å¾—æ›´åŠ ä»€ä¹ˆä¸œè¥¿æ¹›è“ã€‚æœ€è½¯å‘¢ï¼Ÿâ€\n",
      "\n",
      "å¤§è±¡å›ç­”ï¼šâ€œè¿™ä¸ªå˜›ï¼Œæˆ‘ä¹ŸçŸ¥é“ï¼Œä¸–ç•Œä¸Šæœ€è½¯çš„ä¸œè¥¿æ˜¯æ£‰èŠ±ã€‚â€\n",
      "\n",
      "èš‚èšé«˜å…´åœ°è¯´ï¼šâ€œè°¢è°¢å¤§è±¡å…ˆç”Ÿï¼Œé‚£è¯·é—®ï¼Œä¸–ç•Œä¸Šä»€ä¹ˆä¸œè¥¿æœ€èªæ˜å‘¢ï¼Ÿâ€\n",
      "\n",
      "å¤§è±¡æƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œè¿™ä¸ªå˜›ï¼Œæˆ‘å¾—æƒ³æƒ³â€¦â€¦â€\n",
      "\n",
      "èš‚èšç­‰å•Šç­‰å•Šï¼Œç»ˆäºç­‰å¾—ä¸è€çƒ¦äº†ï¼Œå®ƒè¯´ï¼šâ€œå“å‘€ï¼Œå¤§è±¡å…ˆç”Ÿï¼Œæ‚¨æ€ä¹ˆè¿˜ä¸å›ç­”å‘¢ï¼Ÿâ€\n",
      "\n",
      "å¤§è±¡è¯´ï¼šâ€œå“å‘€ï¼Œä¸å¥½æ„æ€ï¼Œæˆ‘å¿˜äº†ï¼Œä¸–ç•Œä¸Šæœ€èªæ˜çš„ä¸œè¥¿æ˜¯â€”â€”ä½ è¿™åªé—®é—®é¢˜çš„èš‚èšï¼â€\n",
      "\n",
      "å“ˆå“ˆï¼Œè¿™ä¸ªç¬‘è¯æœ‰ç‚¹è‡ªå˜²çš„æ„å‘³ï¼Œå¸Œæœ›æ‚¨å–œæ¬¢ï¼"
     ]
    }
   ],
   "source": [
    "# å®‰è£… nest_asyncio\n",
    "! pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "api_key = \"sk-pvdcqtmuphqjgepywixzapshexhxcugkhnrtjnuhdotlvgtx\"\n",
    "base_url = \"https://api.siliconflow.cn/v1\"\n",
    "async def task1():\n",
    "    model = ChatOpenAI(\n",
    "        model=\"THUDM/glm-4-9b-chat\",\n",
    "        temperature=0,\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    chunks = []\n",
    "    \n",
    "    async for chunk in model.astream(\"å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ\"):\n",
    "        chunks.append(chunk)\n",
    "        # åˆ¤æ–­chunks é•¿åº¦ä¸º1çš„æ—¶å€™ï¼Œæ‰“å°chunks[0]\n",
    "        if len(chunks) == 2:\n",
    "            print(chunks[1])\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "async def task2():\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model=\"THUDM/glm-4-9b-chat\",\n",
    "        temperature=0,\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=api_key,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    chunks = []\n",
    "    async for chunk in model.astream(\"ç»™æˆ‘è®²ä¸ªç¬‘è¯ï¼Ÿ\"):\n",
    "        chunks.append(chunk)\n",
    "        # åˆ¤æ–­chunks é•¿åº¦ä¸º1çš„æ—¶å€™ï¼Œæ‰“å°chunks[0]\n",
    "        if len(chunks) == 2:\n",
    "            print(chunks[1])\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "async def main():\n",
    "    # åŒæ­¥è°ƒç”¨\n",
    "    # await task1()\n",
    "    # await task2()\n",
    "    # å¼‚æ­¥è°ƒç”¨ï¼Œå¹¶å‘æ‰§è¡Œ\n",
    "    await asyncio.gather(task1(), task2())\n",
    "    \n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
