{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace 核心组件及应用实战\n",
    "- 1.什么是 Hugging Face?它的目标是什么?\n",
    "- 2.Hugging Face 中包含哪些知名的预训练模型?\n",
    "- 3.如果我们要在 Hugging Face 中下载 BERT，那么\n",
    "    - a.只有一种版本，还是有多种版本可以选择?\n",
    "    - b.每一种版本的 BERT 中，只有一种格式还是有多种格式可以适应多种下游任务?\n",
    "- 4.Hugging Face 库中有哪些有用的组件?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://huggingface.co\n",
    "\n",
    "- Hugging Face 核心组件包括 Transformers、Dataset、Tokenizer，此外还有一些辅助工具，如\n",
    "Accelerate，用于加速深度学习训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、 Hugging Face Transformers 模型介绍\n",
    "- Transformers 是 Hugging Face 的核心组件，主要用于自然语言处理，提供了预训练的语言模型和相关工具，使得研究者和工程师能够轻松的训练和使用海量的 NLP 模型。\n",
    "常用的模型包括 BERT、GPT、XLNet、ROBERTa 等，并提供了模型的各种版本。\n",
    "- 通过 Transformers 库，开发人员可以用这些预训练模型进行文本分类、命名实体识别、机器翻译、问答系统等 NLP 任务。\n",
    "Transformers 库本身还提供方便的 API、实例代码、文档，让开发者学习和使用这些模型都变得非常简单，同时开发者也可以上传自己的预训练模型和 API。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、Hugging Face Dataset\n",
    "- Dataset 是 Hugging Face 的公共数据集仓库，以下是常用的一些数据集(欢迎补充)\n",
    "- 1.SQuAD:Stanford 大学发布的问答数据集\n",
    "- 2.IMBDB:电影评论数据集\n",
    "- 3.CONLL-2003:NER 命名实体识别数据集\n",
    "- 4.GLUE:公共基准测试集\n",
    "- Hugging Face Dataset 简化了数据集的下载、预处理过程，并具备数据集分割、采样和迭代器等功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、Hugging Face Tokenizer\n",
    "- Tokenizer 是 Hugging Face 的分词器，它的任务是将输入文本转换为一个个标记(tokens)，它还能对文本序列进行清洗、截断和填充等预处理，以满足模型的输入要求。\"它的主要功能是将文本分解为更小的单元(通常是词或子词)，并将这些单元映射到数值表示。以下是分词器的简单解释:\n",
    "- 1.文本分割:分词器首先将输入的文本分割成更小的单元。例如，对于句子\"ove NLP\"，分词器可能\n",
    "会将其分割为LOVe"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
